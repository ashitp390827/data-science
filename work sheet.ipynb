{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224627f4-300c-4c58-bb6c-fe11024b7f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e5a2a1-ffa7-44c6-ba74-258c1040f2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223fb245-ee6c-40d7-b84a-0c096d248d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Machine Learning Pipeline for Chicken Shelf-Life Prediction\n",
    "----------------------------------------------------------\n",
    "This script:\n",
    "1. Loads and preprocesses feature datasets.\n",
    "2. Handles class imbalance using SMOTE.\n",
    "3. Standardizes features.\n",
    "4. Trains and tunes models using GridSearchCV.\n",
    "5. Evaluates models on test and shelf-life datasets.\n",
    "6. Saves trained models and evaluation results.\n",
    "\"\"\"\n",
    "\n",
    "# =============================\n",
    "# Imports\n",
    "# =============================\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report,\n",
    "    precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Configurations\n",
    "# =============================\n",
    "DATA_FILE = \"data.xlsx\"\n",
    "SHELF_LIFE_FILE = \"shelf_life_data_f.xlsx\"\n",
    "SCALER_FILE = \"standard_scaler.joblib\"\n",
    "RESULTS_FILE = \"model_evaluation_results_with_roc_conf_matrix.csv\"\n",
    "\n",
    "FEATURES = [\"log-Hue\", \"Saturation\", \"b* lab\"]\n",
    "TARGET = \"Classification_label\"\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Data Loading & Preprocessing\n",
    "# =============================\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load dataset, apply SMOTE, split train/test, and scale features.\"\"\"\n",
    "    # Load dataset\n",
    "    data = pd.read_excel(DATA_FILE)\n",
    "    X = data[FEATURES]\n",
    "    y = data[TARGET]\n",
    "\n",
    "    # Handle class imbalance\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "    # Split into train/test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_resampled, y_resampled, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Save scaler\n",
    "    joblib.dump(scaler, SCALER_FILE)\n",
    "    print(f\"Scaler saved as {SCALER_FILE}\")\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler\n",
    "\n",
    "\n",
    "# def load_shelf_life_data(scaler):\n",
    "#     \"\"\"Load and scale shelf-life dataset for separate evaluation.\"\"\"\n",
    "#     shelf_data = pd.read_excel(SHELF_LIFE_FILE)\n",
    "#     X_shelf = shelf_data[FEATURES]\n",
    "#     y_shelf = shelf_data[\"real_code\"]\n",
    "#     X_shelf_scaled = scaler.transform(X_shelf)\n",
    "#     return X_shelf_scaled, y_shelf\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Model Training & Evaluation\n",
    "# =============================\n",
    "def evaluate_model(name, model, param_grid, X_train, y_train, X_test, y_test, X_shelf, y_shelf):\n",
    "    \"\"\"Train model using GridSearchCV, evaluate, plot, and return results.\"\"\"\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring=\"accuracy\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Confusion matrix heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "    plt.title(f\"Confusion Matrix for {name}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "    # Metrics\n",
    "    precision_per_class = precision_score(y_test, y_pred, average=None).tolist()\n",
    "    recall_per_class = recall_score(y_test, y_pred, average=None).tolist()\n",
    "    f1_per_class = f1_score(y_test, y_pred, average=None).tolist()\n",
    "\n",
    "    weighted_precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "    weighted_recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "    weighted_f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(best_model, X_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "    mean_cv_accuracy, std_cv_accuracy = cv_scores.mean(), cv_scores.std()\n",
    "\n",
    "    # Shelf-life evaluation\n",
    "    y_shelf_pred = best_model.predict(X_shelf)\n",
    "    shelf_life_accuracy = accuracy_score(y_shelf, y_shelf_pred)\n",
    "    print(f\"Shelf-life predictions for {name}: {y_shelf_pred}\")\n",
    "\n",
    "    # ROC Curve\n",
    "    if hasattr(best_model, \"predict_proba\"):\n",
    "        y_test_binarized = label_binarize(y_test, classes=np.unique(y_test))\n",
    "        y_score = best_model.predict_proba(X_test)\n",
    "\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        for i, class_label in enumerate(np.unique(y_test)):\n",
    "            fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_score[:, i])\n",
    "            auc_score = roc_auc_score(y_test_binarized[:, i], y_score[:, i])\n",
    "            plt.plot(fpr, tpr, label=f\"Class {class_label} (AUC = {auc_score:.4f})\")\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], \"k--\")\n",
    "        plt.title(f\"ROC Curve per Class for {name}\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # Save model\n",
    "    model_filename = f\"{name.replace(' ', '_').lower()}_model.joblib\"\n",
    "    joblib.dump(best_model, model_filename)\n",
    "    print(f\"{name} saved as {model_filename}\")\n",
    "\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"Best Params\": grid_search.best_params_,\n",
    "        \"Test Accuracy\": test_accuracy,\n",
    "        \"Confusion Matrix\": conf_matrix.tolist(),\n",
    "        \"Precision (per class)\": precision_per_class,\n",
    "        \"Recall (per class)\": recall_per_class,\n",
    "        \"F1 Score (per class)\": f1_per_class,\n",
    "        \"Weighted Precision\": weighted_precision,\n",
    "        \"Weighted Recall\": weighted_recall,\n",
    "        \"Weighted F1 Score\": weighted_f1,\n",
    "        \"Mean CV Accuracy (10-fold)\": mean_cv_accuracy,\n",
    "        \"Std CV Accuracy (10-fold)\": std_cv_accuracy,\n",
    "        \"Shelf Life Accuracy\": shelf_life_accuracy,\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Main Execution\n",
    "# =============================\n",
    "def main():\n",
    "    # Load and preprocess\n",
    "    X_train, X_test, y_train, y_test, scaler = load_and_preprocess_data()\n",
    "    # X_shelf, y_shelf = load_shelf_life_data(scaler)\n",
    "\n",
    "    # Classifier configurations\n",
    "    param_grids = {\n",
    "        \"Neural Network\": {\n",
    "            \"model\": MLPClassifier(max_iter=3000, random_state=42),\n",
    "            \"param_grid\": {\n",
    "                \"hidden_layer_sizes\": [(50, 50)],\n",
    "                \"activation\": [\"relu\"],\n",
    "                \"solver\": [\"adam\"],\n",
    "                \"alpha\": [0.01],\n",
    "                \"learning_rate\": [\"constant\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Train and evaluate\n",
    "    results = []\n",
    "    for name, config in param_grids.items():\n",
    "        result = evaluate_model(\n",
    "            name, config[\"model\"], config[\"param_grid\"],\n",
    "            X_train, y_train, X_test, y_test, X_shelf, y_shelf\n",
    "        )\n",
    "        results.append(result)\n",
    "\n",
    "    # Save results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(RESULTS_FILE, index=False)\n",
    "    print(f\"Results saved to {RESULTS_FILE}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fe69c1-a2d8-4ca6-aa9e-bc87a49de631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749face2-d68d-46b3-ab99-c797731f831a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'trained_model.joblib'\n",
      "Scaler saved as 'scaler.joblib'\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(best_model, 'trained_model.joblib')\n",
    "print(\"Model saved as 'trained_model.joblib'\")\n",
    "\n",
    "# Export the scaler to a joblib file\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "print(\"Scaler saved as 'scaler.joblib'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684838fd-ed5f-404e-86bd-694c93c7b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00662588-29b7-469d-9a25-6062d57b4c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Regression Pipeline for Shelf-Life Prediction\n",
    "--------------------------------------------\n",
    "This script:\n",
    "1. Loads and preprocesses data.\n",
    "2. Monitors CPU usage to prevent overload.\n",
    "3. Trains and tunes regression models using GridSearchCV.\n",
    "4. Evaluates models with RMSEP, RMSECV, R², and shelf-life dataset.\n",
    "5. Saves results to CSV for comparison.\n",
    "\"\"\"\n",
    "\n",
    "# =============================\n",
    "# Imports\n",
    "# =============================\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import threading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Configurations\n",
    "# =============================\n",
    "DATA_FILE = \"data.xlsx\"\n",
    "SHELF_LIFE_FILE = \"data.xlsx\"\n",
    "RESULTS_FILE = \"regression_model_results_with_tuning.csv\"\n",
    "\n",
    "FEATURES = [\"log-Hue\", \"Saturation\", \"b* lab\"]\n",
    "TARGET = \"mg N/100g\"\n",
    "\n",
    "CPU_THRESHOLD = 85   # % usage limit\n",
    "CHECK_INTERVAL = 5   # seconds\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Data Handling\n",
    "# =============================\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load dataset, split into train/test, and standardize features.\"\"\"\n",
    "    data = pd.read_excel(DATA_FILE)\n",
    "    X = data[FEATURES]\n",
    "    y = data[TARGET]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler\n",
    "\n",
    "\n",
    "# def load_shelf_life_data(scaler):\n",
    "#     \"\"\"Load and scale shelf-life dataset for evaluation.\"\"\"\n",
    "#     shelf_data = pd.read_excel(SHELF_LIFE_FILE)\n",
    "#     X_shelf = shelf_data[FEATURES]\n",
    "#     y_shelf = shelf_data[\"Experimental (N mg/100g)\"]\n",
    "#     X_shelf_scaled = scaler.transform(X_shelf)\n",
    "#     return X_shelf_scaled, y_shelf\n",
    "\n",
    "\n",
    "# =============================\n",
    "# CPU Monitoring\n",
    "# =============================\n",
    "def monitor_cpu():\n",
    "    \"\"\"Monitor CPU usage and stop if usage exceeds threshold.\"\"\"\n",
    "    while True:\n",
    "        cpu_usage = psutil.cpu_percent(interval=1)\n",
    "        if cpu_usage > CPU_THRESHOLD:\n",
    "            print(f\"CPU usage {cpu_usage}% exceeded threshold {CPU_THRESHOLD}%\")\n",
    "            raise SystemExit(\"Terminated due to high CPU usage.\")\n",
    "        time.sleep(CHECK_INTERVAL)\n",
    "\n",
    "\n",
    "def start_cpu_monitor():\n",
    "    \"\"\"Start CPU monitoring in a background thread.\"\"\"\n",
    "    cpu_thread = threading.Thread(target=monitor_cpu, daemon=True)\n",
    "    cpu_thread.start()\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Model Training & Evaluation\n",
    "# =============================\n",
    "def evaluate_regressor(name, model, param_grid, X_train, y_train, X_test, y_test, X_shelf, y_shelf):\n",
    "    \"\"\"Perform GridSearchCV, evaluate metrics, and return results.\"\"\"\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring=\"r2\",\n",
    "        n_jobs=2  # Protect CPU\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"Best parameters for {name}: {best_params}\")\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmsep = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # RMSECV (cross-validation)\n",
    "    rmsecv_scores = -cross_val_score(best_model, X_train, y_train, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "    rmsecv = np.sqrt(rmsecv_scores.mean())\n",
    "    rmsecv_std = rmsecv_scores.std()\n",
    "\n",
    "    # Cross-validated R²\n",
    "    r2_cv_scores = cross_val_score(best_model, X_train, y_train, cv=10, scoring=\"r2\")\n",
    "    r2_cv, r2_cv_std = r2_cv_scores.mean(), r2_cv_scores.std()\n",
    "\n",
    "    # # Shelf-life dataset evaluation\n",
    "    # y_shelf_pred = best_model.predict(X_shelf)\n",
    "    # shelf_r2 = r2_score(y_shelf, y_shelf_pred)\n",
    "    # mse_shelf = mean_squared_error(y_shelf, y_shelf_pred)\n",
    "    # rmsep_shelf = np.sqrt(mse_shelf)\n",
    "\n",
    "    # print(f\"Shelf-life predictions for {name}: {y_shelf_pred}\")\n",
    "\n",
    "    # print(\n",
    "    #     f\"Model: {name}, \"\n",
    "    #     f\"Test RMSEP: {rmsep:.4f}, RMSECV: {rmsecv:.4f} ± {rmsecv_std:.4f}, \"\n",
    "    #     f\"Test R²: {r2:.4f}, CV R²: {r2_cv:.4f} ± {r2_cv_std:.4f}, \"\n",
    "    #     f\"Shelf Life R²: {shelf_r2:.4f}, Shelf RMSEP: {rmsep_shelf:.4f}\"\n",
    "    # )\n",
    "\n",
    "    return {\n",
    "        \"Features\": FEATURES,\n",
    "        \"Model\": name,\n",
    "        \"Best Params\": best_params,\n",
    "        \"RMSEP\": rmsep,\n",
    "        \"RMSECV\": rmsecv,\n",
    "        \"RMSECV Std Dev\": rmsecv_std,\n",
    "        \"Test R²\": r2,\n",
    "        \"Cross-validated R²\": r2_cv,\n",
    "        \"R² CV Std Dev\": r2_cv_std,\n",
    "        # \"Shelf Life R²\": shelf_r2,\n",
    "        # \"Shelf RMSEP\": rmsep_shelf,\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Main Execution\n",
    "# =============================\n",
    "def main():\n",
    "    # Start CPU monitoring\n",
    "    start_cpu_monitor()\n",
    "\n",
    "    # Load datasets\n",
    "    X_train, X_test, y_train, y_test, scaler = load_and_preprocess_data()\n",
    "    # X_shelf, y_shelf = load_shelf_life_data(scaler)\n",
    "\n",
    "    # Parameter grids\n",
    "    param_grids = {\n",
    "        \"Neural Network\": {\n",
    "            \"hidden_layer_sizes\": [(50, 50)],\n",
    "            \"activation\": [\"relu\"],\n",
    "            \"solver\": [\"adam\"],\n",
    "            \"alpha\": [0.01],\n",
    "            \"learning_rate\": [\"constant\"],\n",
    "            \"max_iter\": [3000],\n",
    "\n",
    "    }\n",
    "\n",
    "    regressors = {\n",
    "        \"Neural Network\": MLPRegressor(random_state=42, max_iter=3000),\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Train & evaluate\n",
    "    for name, model in regressors.items():\n",
    "        if name in param_grids:\n",
    "            print(f\"\\n=== Training {name} ===\")\n",
    "            result = evaluate_regressor(\n",
    "                name, model, param_grids[name],\n",
    "                X_train, y_train, X_test, y_test,\n",
    "                X_shelf, y_shelf\n",
    "            )\n",
    "            results.append(result)\n",
    "\n",
    "    # Save results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(RESULTS_FILE, index=False)\n",
    "    print(f\"\\nResults saved to {RESULTS_FILE}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567dc862-f95a-4ea7-856f-698d9454268c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c22d26-ea6e-41bf-8646-74cdea180e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64097346-2209-4b34-9985-f263faa2819d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
